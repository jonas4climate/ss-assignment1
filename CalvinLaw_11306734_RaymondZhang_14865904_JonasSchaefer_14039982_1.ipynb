{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T17:34:45.867715200Z",
     "start_time": "2023-11-19T17:34:45.848110600Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import njit, jit, prange\n",
    "from scipy.stats import qmc\n",
    "from scipy.stats import norm, ttest_ind\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e90d24cd043cf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T17:34:47.178953500Z",
     "start_time": "2023-11-19T17:34:47.156818200Z"
    }
   },
   "outputs": [],
   "source": [
    "SAVE_DIR = 'media/final'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "SEED = 35\n",
    "BE_REPRODUCIBLE = True\n",
    "# Code evaluation estimates for MacBook Air 2020 (M1, 8GB RAM) running conda environment with python 3.11.5\n",
    "# Non-reproducible ≈ 10min\n",
    "# Reproducible ≈ 25min\n",
    "PARALLELIZE = not BE_REPRODUCIBLE\n",
    "\n",
    "\n",
    "@njit\n",
    "def set_seed(value):\n",
    "    np.random.seed(value)\n",
    "\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# Complex plane boundaries\n",
    "X_LIM = (-2, 0.75)\n",
    "Y_LIM = (-1.25, 1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76151501214190e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:15:06.433172Z",
     "start_time": "2023-11-19T18:15:06.411851600Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def mandelbrot(z, max_iter) -> int:\n",
    "    \"\"\"\n",
    "    Determine if a complex point is in the Mandelbrot set.\n",
    "    :param z: Complex point\n",
    "    :param max_iter: Maximum number of iterations to determine set membership\n",
    "    :return: Iteration count when z escapes the set, `max_iter` if it doesn't\n",
    "    \"\"\"\n",
    "    c = z\n",
    "    for i in range(max_iter):\n",
    "        if np.abs(z) > 2:\n",
    "            return i\n",
    "        z = z*z + c\n",
    "    return max_iter\n",
    "\n",
    "\n",
    "def bootstrap_confidence_interval(data, num_bootstrap_iters=1000, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Compute the bootstrap confidence interval for a dataset.\n",
    "    :param data: Dataset\n",
    "    :param num_bootstrap_iters: Number of bootstrap iterations\n",
    "    :param confidence_level: Confidence level\n",
    "    :return: Calculated percentile interval\n",
    "    \"\"\"\n",
    "    bootstrap_means = np.empty(num_bootstrap_iters)\n",
    "    for i in range(num_bootstrap_iters):\n",
    "        bootstrap_sample = np.random.choice(data, size=len(data), replace=True)\n",
    "        bootstrap_means[i] = np.mean(bootstrap_sample)\n",
    "    return np.percentile(bootstrap_means, [(1 - confidence_level) / 2 * 100, (1 + confidence_level) / 2 * 100])\n",
    "\n",
    "\n",
    "def hypothesis_testing(data1, data2, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform hypothesis testing to compare two datasets.\n",
    "    :param data1: First dataset\n",
    "    :param data2: Second dataset\n",
    "    :param alpha: Significance level\n",
    "    :return: Test statistic and p-value\n",
    "    \"\"\"\n",
    "    stat, p_value = ttest_ind(data1, data2)\n",
    "    if p_value < alpha:\n",
    "        print(\"Reject the null hypothesis, indicating a significant difference between the two samples.\")\n",
    "    else:\n",
    "        print(\"Fail to reject the null hypothesis, indicating no significant difference between the two samples.\")\n",
    "    return stat, p_value\n",
    "\n",
    "\n",
    "def mandelbrot_statistics(c_points, counts, method, previous_stats=None, tolerance=0.01, print_output=True):\n",
    "    \"\"\"\n",
    "    Generate statistics of Mandelbrot set points.\n",
    "    :param c_points: Complex points\n",
    "    :param counts: Iteration counts for each point\n",
    "    :param method: Method used for generating the points\n",
    "    :param previous_stats: Previous statistical data for comparunrison\n",
    "    :param tolerance: Tolerance level for determining significant changes\n",
    "    :return: Statistical results\n",
    "    \"\"\"\n",
    "    total_area = abs(X_LIM[0] - X_LIM[1]) * abs(Y_LIM[0] - Y_LIM[1])\n",
    "    max_iter = np.max(counts)\n",
    "    mandelbrot_ratio = np.count_nonzero(counts == max_iter) / counts.size\n",
    "    mandelbrot_area = total_area * mandelbrot_ratio\n",
    "    inside = counts == max_iter\n",
    "    area_samples = inside / (counts.size / total_area)\n",
    "    sample_mean = np.mean(area_samples)\n",
    "    sample_std = np.std(area_samples, ddof=1)  # ddof=1 for sample std. dev.\n",
    "    confidence_interval = norm.interval(\n",
    "        0.95, loc=sample_mean, scale=sample_std / np.sqrt(area_samples.size))\n",
    "\n",
    "    has_converged = False\n",
    "    if previous_stats:\n",
    "        mean_change = abs(previous_stats['mean'] - sample_mean)\n",
    "        std_change = abs(previous_stats['std'] - sample_std)\n",
    "        if mean_change < tolerance and std_change < tolerance:\n",
    "            has_converged = True\n",
    "\n",
    "    stats = {\n",
    "        'area': mandelbrot_area,\n",
    "        'mean': sample_mean,\n",
    "        'std': sample_std,\n",
    "        'conf_int': confidence_interval,\n",
    "        'has_converged': has_converged\n",
    "    }\n",
    "\n",
    "    # Print only if there's a significant change or upon convergence\n",
    "    if print_output and (not previous_stats or mean_change >= tolerance or std_change >= tolerance or has_converged):\n",
    "        print(f\"Mandelbrot area: {mandelbrot_area} using {method} method\")\n",
    "        print(f\"Mean estimated area: {sample_mean}\")\n",
    "        print(f\"Standard deviation of estimated area: {sample_std}\")\n",
    "        print(\n",
    "            f\"95% Confidence interval for the estimated area: {confidence_interval}\")\n",
    "        if has_converged:\n",
    "            print(\"Stopping criterion met - simulation has converged.\")\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def plot_mandelbrot(c_points, counts, method, max_iter=250, save=True, show=True, title_version=False, cmap='binary'):\n",
    "    \"\"\"\n",
    "    Basic functionality to plot the mandelbrot set.\n",
    "    :param c_points: Complex points\n",
    "    :param counts: Iteration counts for each point\n",
    "    :param method: Method used for generating the points\n",
    "    :param max_iter: Maximum number of iterations\n",
    "    :param save: Whether to save the plot\n",
    "    :param show: Whether to show the plot\n",
    "    \"\"\"\n",
    "    N = int(np.sqrt(len(counts)))\n",
    "    if title_version:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "    else:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "\n",
    "    match(method):\n",
    "        case 'fixed':\n",
    "            image_data = counts.reshape(N, N)\n",
    "            plt.imshow(image_data, cmap=cmap, extent=[*X_LIM, *Y_LIM])\n",
    "        case 'monte-carlo' | 'lhs' | 'orthogonal' | 'random':\n",
    "            image_data = np.zeros((N, N), dtype=np.int32)\n",
    "            for i, c_point in enumerate(c_points):\n",
    "                x_index = int(\n",
    "                    (c_point.real - X_LIM[0]) / (X_LIM[1] - X_LIM[0]) * N)\n",
    "                y_index = int(\n",
    "                    (c_point.imag - Y_LIM[0]) / (Y_LIM[1] - Y_LIM[0]) * N)\n",
    "                image_data[x_index, y_index] = counts[i]\n",
    "            image_data = np.rot90(image_data)\n",
    "            plt.imshow(image_data, cmap=cmap, extent=[*X_LIM, *Y_LIM])\n",
    "        case _:\n",
    "            raise ValueError('Invalid argument, method does not exist.')\n",
    "    if title_version:\n",
    "        plt.axis('off')\n",
    "    else:\n",
    "        plt.title(\n",
    "            f'Mandelbrot set generated using {len(counts):.1e} points\\nusing the {method} method')\n",
    "        plt.colorbar(shrink=0.85, label='# iterations')\n",
    "        plt.xlabel('x (real)')\n",
    "        plt.ylabel('y (imaginary)')\n",
    "\n",
    "    if save:\n",
    "        if title_version:\n",
    "            plt.savefig(\n",
    "                f'{SAVE_DIR}/mandelbrot_{method}_{max_iter}_{N}_title.png', dpi=600, bbox_inches='tight', pad_inches=0)\n",
    "        else:\n",
    "            plt.savefig(\n",
    "                f'{SAVE_DIR}/mandelbrot_{method}_{max_iter}_{N}.png', dpi=300)\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "@jit(nopython=True, parallel=PARALLELIZE)\n",
    "def mandelbrot_parallel_fixed(max_iter=250, size=1_000):\n",
    "    \"\"\"\n",
    "    Generate points on a fixed grid and evaluate their mandelbrot set membership.\n",
    "    :param max_iter: Maximum iterations\n",
    "    :param size: Number of points\n",
    "    :return: Complex points and iteration counts\n",
    "    \"\"\"\n",
    "    real_parts, imag_parts = np.linspace(\n",
    "        *X_LIM, size), np.linspace(*Y_LIM, size)\n",
    "    c_points = np.zeros((size, size), dtype=np.complex128)\n",
    "    counts = np.zeros((size, size), dtype=np.int32)\n",
    "\n",
    "    for i in prange(size):\n",
    "        for j in prange(size):\n",
    "            z = complex(real_parts[j], imag_parts[i])\n",
    "            c_points[i, j] = z\n",
    "            counts[i, j] = mandelbrot(z, max_iter)\n",
    "    return c_points.flatten(), counts.flatten()\n",
    "\n",
    "\n",
    "@jit(nopython=True, parallel=PARALLELIZE)\n",
    "def mandelbrot_parallel_mc(max_iter=250, n_points=1_000_000):\n",
    "    \"\"\" \n",
    "    Generate random points uniformly and evaluate their mandelbrot set membership.\n",
    "    :param max_iter: Maximum iterations\n",
    "    :param n_points: Number of points\n",
    "    :return: Complex points, iteration counts and members of the set\n",
    "    \"\"\"\n",
    "    real_parts = np.random.uniform(low=X_LIM[0], high=X_LIM[1], size=n_points)\n",
    "    imag_parts = np.random.uniform(low=Y_LIM[0], high=Y_LIM[1], size=n_points)\n",
    "    c_points = np.stack((real_parts, imag_parts), axis=-1).view(np.complex128)\n",
    "    counts = np.zeros((n_points), dtype=np.int32)\n",
    "\n",
    "    for i in prange(n_points):\n",
    "        z = c_points[i]\n",
    "        counts[i] = mandelbrot(z, max_iter)\n",
    "    return c_points.flatten(), counts.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bd589fc7f237e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:15:20.832445800Z",
     "start_time": "2023-11-19T18:15:18.375409Z"
    }
   },
   "outputs": [],
   "source": [
    "points_fixed, counts_fixed = mandelbrot_parallel_fixed()\n",
    "mandelbrot_statistics(points_fixed, counts_fixed, method='fixed')\n",
    "plot_mandelbrot(points_fixed, counts_fixed, method='fixed', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_fixed, counts_fixed = mandelbrot_parallel_fixed(size=2_000)\n",
    "plot_mandelbrot(points_fixed, counts_fixed, method='fixed',\n",
    "                save=True, title_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e068f358ca8297a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:15:18.372986300Z",
     "start_time": "2023-11-19T18:15:12.401080600Z"
    }
   },
   "outputs": [],
   "source": [
    "points_mc, counts_mc = mandelbrot_parallel_mc()\n",
    "mandelbrot_statistics(points_mc, counts_mc, method='monte-carlo')\n",
    "plot_mandelbrot(points_mc, counts_mc, method='monte-carlo', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f02d5d745798a01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:16:12.199166400Z",
     "start_time": "2023-11-19T18:16:12.178046700Z"
    }
   },
   "outputs": [],
   "source": [
    "def latin_hypercube_samples(n_samples, d=2):\n",
    "    sampler = qmc.LatinHypercube(d=d)\n",
    "    return np.array(sampler.random(n=n_samples))\n",
    "\n",
    "\n",
    "@jit(nopython=True, parallel=PARALLELIZE)\n",
    "def latin_hypercube_sampling(max_iter, n_samples, sample):\n",
    "    \"\"\"\n",
    "    Perform Latin Hypercube Sampling to estimate the area of the Mandelbrot set.\n",
    "\n",
    "    Latin Hypercube Sampling improves sample diversity by ensuring that each sample\n",
    "    is the only one in each axis-aligned hyperplane. It is particularly useful in \n",
    "    multi-dimensional Monte Carlo simulations where it reduces the variability of the results.\n",
    "\n",
    "    :param max_iter: Maximum iterations \n",
    "    :param n_samples: Number of samples\n",
    "    :return: A tuple of sampled complex points and their Mandelbrot iteration counts\n",
    "    \"\"\"\n",
    "    range_real = X_LIM[1] - X_LIM[0]\n",
    "    range_imag = Y_LIM[1] - Y_LIM[0]\n",
    "    points = np.zeros((n_samples, 2))\n",
    "    points[:, 0] = X_LIM[0] + sample[:, 0] * range_real\n",
    "    points[:, 1] = Y_LIM[0] + sample[:, 1] * range_imag\n",
    "    c_points = points[:, 0] + 1j * points[:, 1]\n",
    "    counts = np.zeros((n_samples), dtype=np.int32)\n",
    "    for i in prange(n_samples):\n",
    "        counts[i] = mandelbrot(c_points[i], max_iter)\n",
    "    return c_points, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d17a88462fd6f86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:16:14.812932Z",
     "start_time": "2023-11-19T18:16:14.773079100Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True, parallel=PARALLELIZE)\n",
    "def stratified_sampling(max_iter, n_strata, samples_per_stratum):\n",
    "    \"\"\"\n",
    "    Utilize stratified sampling to estimate the Mandelbrot set area.\n",
    "\n",
    "    Stratified sampling ensures that the samples are evenly distributed across the \n",
    "    domain of interest, which can lead to more accurate and reliable estimates of the Mandelbrot set area.\n",
    "\n",
    "    :param max_iter: Maximum iterations for Mandelbrot calculations.\n",
    "    :param n_strata: Number of strata to divide the sample space.\n",
    "    :param samples_per_stratum: Number of samples per stratum.\n",
    "    :return: A tuple of sampled complex points and their Mandelbrot iteration counts.\n",
    "    \"\"\"\n",
    "    real_strata_edges = np.linspace(*X_LIM, n_strata + 1)\n",
    "    imag_strata_edges = np.linspace(*Y_LIM, n_strata + 1)\n",
    "\n",
    "    c_points = np.zeros(n_strata * n_strata *\n",
    "                        samples_per_stratum, dtype=np.complex128)\n",
    "    counts = np.zeros(n_strata * n_strata *\n",
    "                      samples_per_stratum, dtype=np.int32)\n",
    "\n",
    "    index = 0\n",
    "    for i in prange(n_strata):\n",
    "        for j in prange(n_strata):\n",
    "            real_samples = np.random.uniform(\n",
    "                real_strata_edges[i], real_strata_edges[i+1], samples_per_stratum)\n",
    "            imag_samples = np.random.uniform(\n",
    "                imag_strata_edges[j], imag_strata_edges[j+1], samples_per_stratum)\n",
    "\n",
    "            for k in prange(samples_per_stratum):\n",
    "                c = real_samples[k] + 1j * imag_samples[k]\n",
    "                c_points[index] = c\n",
    "                counts[index] = mandelbrot(c, max_iter)\n",
    "                index += 1\n",
    "\n",
    "    return c_points, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e0d2a49f6137e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:23:11.393983500Z",
     "start_time": "2023-11-19T18:23:11.370155100Z"
    }
   },
   "outputs": [],
   "source": [
    "def investigate_convergence_2d(iter_range, sample_sizes, improvement_method=None):\n",
    "    \"\"\"\n",
    "    Explore the convergence behavior of the Mandelbrot set area estimation as a function of\n",
    "    iterations and sample size, and assess the efficacy of different sampling methods.\n",
    "\n",
    "    :param iter_range: A range of iteration counts to test.\n",
    "    :param sample_sizes: A range of sample sizes to test.\n",
    "    :param improvement_method: The sampling method used to improve convergence.\n",
    "    :return: A DataFrame with the area estimates and associated statistics.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "\n",
    "    progress_bar = tqdm(total=len(iter_range) * len(sample_sizes),\n",
    "                        desc='Convergence Analysis', leave=True, position=0)\n",
    "    # Iterate over all combinations of iteration counts and sample sizes.\n",
    "    for max_iter in iter_range:\n",
    "        for n_samples in sample_sizes:\n",
    "            # Ensure a balance between iterations and sample size to compare their errors.\n",
    "            if n_samples >= max_iter:\n",
    "                # Apply the appropriate sampling method.\n",
    "                if improvement_method == 'lhs':\n",
    "                    sample = latin_hypercube_samples(n_samples)\n",
    "                    c_points, counts = latin_hypercube_sampling(\n",
    "                        max_iter, n_samples, sample)\n",
    "                elif improvement_method == 'stratified':\n",
    "                    n_strata = int(np.sqrt(n_samples))\n",
    "                    samples_per_stratum = n_samples // n_strata\n",
    "                    c_points, counts = stratified_sampling(\n",
    "                        max_iter, n_strata, samples_per_stratum)\n",
    "                else:\n",
    "                    c_points, counts = mandelbrot_parallel_mc(\n",
    "                        max_iter, n_samples)\n",
    "\n",
    "                # Calculate statistics for the current configuration.\n",
    "                stats = mandelbrot_statistics(\n",
    "                    c_points, counts, method=improvement_method or 'monte-carlo', print_output=False)\n",
    "                results_list.append({\n",
    "                    'iterations': max_iter,\n",
    "                    'samples': n_samples,\n",
    "                    'estimated_area': stats['area'],\n",
    "                    'std_dev': stats['std'],\n",
    "                    'conf_int_low': stats['conf_int'][0],\n",
    "                    'conf_int_high': stats['conf_int'][1]\n",
    "                })\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix(\n",
    "                {'max_iter': max_iter, 'n_samples': n_samples}, refresh=True)\n",
    "\n",
    "    # Compile the results into a DataFrame for further analysis.\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    # Analyze and visualize the convergence behavior.\n",
    "    postfix = improvement_method or ''\n",
    "    visualize_convergence_analysis(\n",
    "        results_df, iter_range, sample_sizes, postfix=postfix)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def visualize_convergence_analysis(results_df, iter_range, sample_sizes, postfix=''):\n",
    "    \"\"\"\n",
    "    Generate plots to visualize the error in estimated area as a function of iteration count\n",
    "    and sample size, based on reference areas from the convergence study.\n",
    "\n",
    "    :param results_df: DataFrame containing the results of the convergence study.\n",
    "    :param iter_range: Range of iteration counts used in the study.\n",
    "    :param sample_sizes: Range of sample sizes used in the study.\n",
    "    \"\"\"\n",
    "    largest_i = max(iter_range)\n",
    "    reference_area_by_iter = results_df[results_df['samples'] == max(\n",
    "        sample_sizes)].set_index('iterations')['estimated_area']\n",
    "    reference_area_by_samples = results_df[results_df['iterations'] == max(\n",
    "        iter_range)].set_index('samples')['estimated_area']\n",
    "\n",
    "    plot_error_by_iteration(results_df, iter_range,\n",
    "                            sample_sizes, reference_area_by_iter, postfix=postfix)\n",
    "    plot_error_by_sample_size(results_df, iter_range,\n",
    "                              sample_sizes, reference_area_by_samples, postfix=postfix)\n",
    "\n",
    "\n",
    "def plot_error_by_iteration(results_df, iter_range, sample_sizes, reference_area_by_iter, postfix=''):\n",
    "    \"\"\"\n",
    "    Plot the error in estimated area as a function of iteration count.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    for sample_size in sample_sizes:\n",
    "        errors = calculate_errors_by_iteration(\n",
    "            results_df, iter_range, sample_size, reference_area_by_iter)\n",
    "        plt.plot(iter_range, errors, '-o', label=f'Samples={sample_size}')\n",
    "    plt.xlabel('Iteration count')\n",
    "    plt.ylabel('Error from reference area by iteration')\n",
    "    plt.legend()\n",
    "    plt.title('Error analysis for varying iteration counts')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\n",
    "        f'{SAVE_DIR}/plot_error_by_iteration_{len(iter_range)}_{len(sample_sizes)}_{postfix}.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_error_by_sample_size(results_df, iter_range, sample_sizes, reference_area_by_samples, postfix=''):\n",
    "    \"\"\"\n",
    "    Plot the error in estimated area as a function of sample size.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    for iter_count in iter_range:\n",
    "        errors = calculate_errors_by_sample_size(\n",
    "            results_df, iter_count, sample_sizes, reference_area_by_samples)\n",
    "        plt.plot(sample_sizes, errors, '-o', label=f'Iterations={iter_count}')\n",
    "    plt.xlabel('Sample size as a factor of 1000')\n",
    "    plt.xticks(sample_sizes, [f'{s/1000}' for s in sample_sizes])\n",
    "    plt.ylabel('Error from reference area by sample size')\n",
    "    plt.legend()\n",
    "    plt.title('Error analysis for varying sample sizes')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\n",
    "        f'{SAVE_DIR}/plot_error_by_sample_size_{len(iter_range)}_{len(sample_sizes)}_{postfix}.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_errors_by_iteration(results_df, iter_range, sample_size, reference_area_by_iter):\n",
    "    \"\"\"\n",
    "    Calculate errors in estimated area for a fixed sample size across different iteration counts.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    for iter_count in iter_range:\n",
    "        if not results_df[(results_df['iterations'] == iter_count) & (results_df['samples'] == sample_size)].empty:\n",
    "            estimated_area = results_df[(results_df['iterations'] == iter_count) & (\n",
    "                results_df['samples'] == sample_size)]['estimated_area'].iloc[0]\n",
    "            ref_area = reference_area_by_iter[iter_count]\n",
    "            errors.append(abs(estimated_area - ref_area))\n",
    "    return errors\n",
    "\n",
    "\n",
    "def calculate_errors_by_sample_size(results_df, iter_count, sample_sizes, reference_area_by_samples):\n",
    "    \"\"\"\n",
    "    Calculate errors in estimated area for a fixed iteration count across different sample sizes.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    for sample_size in sample_sizes:\n",
    "        if not results_df[(results_df['iterations'] == iter_count) & (results_df['samples'] == sample_size)].empty:\n",
    "            estimated_area = results_df[(results_df['iterations'] == iter_count) & (\n",
    "                results_df['samples'] == sample_size)]['estimated_area'].iloc[0]\n",
    "            ref_area = reference_area_by_samples[sample_size]\n",
    "            errors.append(abs(estimated_area - ref_area))\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea561e44fbb6cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:23:23.981143100Z",
     "start_time": "2023-11-19T18:23:16.023067700Z"
    }
   },
   "outputs": [],
   "source": [
    "iter_range = np.geomspace(100, 500, num=5).astype(int)\n",
    "sample_sizes = np.geomspace(100000, 500000, num=5).astype(int)\n",
    "results_df = investigate_convergence_2d(iter_range, sample_sizes)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "# Visualizes convergence of set area with increasing iterations\n",
    "for samples in sample_sizes:\n",
    "    subset = results_df[results_df['samples'] == samples]\n",
    "    plt.plot(subset['iterations'], subset['estimated_area'],\n",
    "             label=f'Samples={samples}')\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Estimated area')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Convergence of estimated area with increasing iterations')\n",
    "plt.savefig(\n",
    "    f'{SAVE_DIR}/convergence_of_estimated_area_with_increasing_iterations.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf925260ba3f851",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:23:29.657205Z",
     "start_time": "2023-11-19T18:23:29.432836300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Analyzes error from reference area for iteration counts\n",
    "largest_i = max(iter_range)\n",
    "reference_area = results_df[results_df['iterations']\n",
    "                            == largest_i]['estimated_area'].iloc[0]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "# Compare error as function of iteration count to largest iteration count\n",
    "for samples in sample_sizes:\n",
    "    errors = []\n",
    "    for i in iter_range:\n",
    "        if i < largest_i:\n",
    "            current_area = results_df[(results_df['iterations'] == i) & (\n",
    "                results_df['samples'] == samples)]['estimated_area'].iloc[0]\n",
    "            error = abs(current_area - reference_area)\n",
    "            errors.append(error)\n",
    "    plt.plot(iter_range[:-1], errors, label=f'Samples={samples}')\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Error from reference area')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Error analysis for finite iterations')\n",
    "plt.savefig(f'{SAVE_DIR}/error_analysis_for_finite_iterations.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c043cc9c18736f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:03:15.402695700Z",
     "start_time": "2023-11-19T17:35:24.505929100Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df = investigate_convergence_2d(\n",
    "    iter_range, sample_sizes, improvement_method='stratified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06cdd404740f8a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:24:11.373865700Z",
     "start_time": "2023-11-19T18:23:43.338707300Z"
    }
   },
   "outputs": [],
   "source": [
    "def orthogonal_sampling(max_iter, n_samples, d=2):\n",
    "    \"\"\"\n",
    "    Generate orthogonal samples within the Mandelbrot set limits. Orthogonal sampling is based on Latin Hypercube Sampling (LHS) but imposes a more regular distribution.\n",
    "    :param max_iter: maximum number of iterations per sample\n",
    "    :param n_samples: Total number of points to sample\n",
    "    :param d: Dimension of the sampling grid, default is 2D\n",
    "    :return: A tuple of complex points and their respective Mandelbrot set iteration counts\n",
    "    \"\"\"\n",
    "    intervals = np.linspace(0, 1, int(np.sqrt(n_samples)) + 1)\n",
    "    points = np.array(np.meshgrid(*([intervals]*d))).T.reshape(-1, d)\n",
    "    np.random.shuffle(points)\n",
    "    points = points[:n_samples]\n",
    "\n",
    "    range_real = X_LIM[1] - X_LIM[0]\n",
    "    range_imag = Y_LIM[1] - Y_LIM[0]\n",
    "    sampled_points = np.zeros((n_samples, 2))\n",
    "    sampled_points[:, 0] = X_LIM[0] + points[:, 0] * range_real\n",
    "    sampled_points[:, 1] = Y_LIM[0] + points[:, 1] * range_imag\n",
    "    c_points = sampled_points[:, 0] + 1j * sampled_points[:, 1]\n",
    "    counts = np.array([mandelbrot(c, max_iter) for c in c_points])\n",
    "    return c_points, counts\n",
    "\n",
    "\n",
    "def antithetic_variates(c_points, counts, max_iter):\n",
    "    \"\"\"\n",
    "    Use antithetic variates to potentially reduce variance in the Monte Carlo simulation. This function takes the negation of complex points as antithetic variates.\n",
    "    :param c_points: Array of complex points sampled\n",
    "    :param counts: Array of iteration counts\n",
    "    :param max_iter: Maximum iterations for set membership\n",
    "    :return: Averaged counts combining original and antithetic variates\n",
    "    \"\"\"\n",
    "    antithetic_points = -c_points\n",
    "    antithetic_counts = np.array([mandelbrot(c, max_iter)\n",
    "                                 for c in antithetic_points])\n",
    "    # Take the average of normal and antithetic counts\n",
    "    return (counts + antithetic_counts) / 2\n",
    "\n",
    "\n",
    "def run_simulation(max_iter, n_samples, sampling_method, sim_runs=10):\n",
    "    \"\"\"\n",
    "    Run multiple simulations of Mandelbrot set calculations using a specified sampling method.\n",
    "    :param max_iter: Maximum iterations for set membership\n",
    "    :param n_samples: Number of samples\n",
    "    :param sampling_method: The method used for sampling points ('random', 'lhs', 'orthogonal')\n",
    "    :param sim_runs: Number of simulations to run\n",
    "    :return: A list of statistical summaries from each simulation run\n",
    "    \"\"\"\n",
    "    stats_list = []\n",
    "    for _ in range(sim_runs):\n",
    "        if sampling_method == 'random':\n",
    "            c_points, counts = mandelbrot_parallel_mc(max_iter, n_samples)\n",
    "            counts = antithetic_variates(\n",
    "                c_points, counts, max_iter)  # Use antithetic variates\n",
    "        elif sampling_method == 'lhs':\n",
    "            sample = latin_hypercube_samples(n_samples)\n",
    "            c_points, counts = latin_hypercube_sampling(\n",
    "                max_iter, n_samples, sample)\n",
    "        elif sampling_method == 'orthogonal':\n",
    "            c_points, counts = orthogonal_sampling(max_iter, n_samples)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sampling method: {sampling_method}\")\n",
    "        stats_list.append(mandelbrot_statistics(\n",
    "            c_points, counts, sampling_method))\n",
    "    return stats_list\n",
    "\n",
    "\n",
    "def investigate_convergence_3d(iter_range, sample_sizes, methods):\n",
    "    \"\"\"\n",
    "    Investigate the convergence of estimated Mandelbrot set area across different iteration ranges and sampling sizes.\n",
    "    :param iter_range: The range of iteration counts to test.\n",
    "    :param sample_sizes: The range of sample sizes to test.\n",
    "    :param methods: The sampling methods to use in the investigation.\n",
    "    :return: A dictionary with the mean and standard deviation of estimated areas for each method.\n",
    "    \"\"\"\n",
    "    mean_estimated_areas = {method: [] for method in methods}\n",
    "    std_dev_estimated_areas = {method: [] for method in methods}\n",
    "\n",
    "    for max_iter in iter_range:\n",
    "        for method in methods:\n",
    "            mean_area_for_iter = []\n",
    "            std_dev_for_iter = []\n",
    "\n",
    "            for n_samples in sample_sizes:\n",
    "                if method == 'random':\n",
    "                    c_points, counts = mandelbrot_parallel_mc(\n",
    "                        max_iter, n_samples)\n",
    "                elif method == 'lhs':\n",
    "                    sample = latin_hypercube_samples(n_samples)\n",
    "                    c_points, counts = latin_hypercube_sampling(\n",
    "                        max_iter, n_samples, sample)\n",
    "                elif method == 'orthogonal':\n",
    "                    c_points, counts = orthogonal_sampling(max_iter, n_samples)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown sampling method: {method}\")\n",
    "\n",
    "                stats = mandelbrot_statistics(\n",
    "                    c_points, counts, method, print_output=False)\n",
    "\n",
    "                mean_area_for_iter.append(stats['mean'])\n",
    "                std_dev_for_iter.append(stats['std'])\n",
    "\n",
    "            mean_estimated_areas[method].append(np.mean(mean_area_for_iter))\n",
    "            std_dev_estimated_areas[method].append(np.mean(std_dev_for_iter))\n",
    "\n",
    "    return mean_estimated_areas, std_dev_estimated_areas\n",
    "\n",
    "\n",
    "methods = ['random', 'lhs', 'orthogonal']\n",
    "mean_areas, std_dev_areas = investigate_convergence_3d(\n",
    "    iter_range, sample_sizes, methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for method in methods:\n",
    "    plt.plot(iter_range, mean_areas[method], '-o', label=method)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Mean estimated area')\n",
    "plt.title('Convergence of mean estimated area')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for method in methods:\n",
    "    plt.plot(iter_range, std_dev_areas[method], '-o', label=method)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Standard deviation of estimated area')\n",
    "plt.title('Convergence of std. dev. of estimated area')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/convergence_of_mean_and_std_dev.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(sampling_method, max_iter, sample_size, num_runs):\n",
    "    area_estimates = []\n",
    "    for _ in range(num_runs):\n",
    "        if sampling_method == 'random':\n",
    "            c_points, counts = mandelbrot_parallel_mc(max_iter, sample_size)\n",
    "        elif sampling_method == 'lhs':\n",
    "            sample = latin_hypercube_samples(sample_size)\n",
    "            c_points, counts = latin_hypercube_sampling(\n",
    "                max_iter, sample_size, sample)\n",
    "        elif sampling_method == 'orthogonal':\n",
    "            c_points, counts = orthogonal_sampling(max_iter, sample_size)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sampling method: {method}\")\n",
    "\n",
    "        area = mandelbrot_statistics(\n",
    "            c_points, counts, sampling_method, print_output=False)['area']\n",
    "        area_estimates.append(area)\n",
    "\n",
    "    return area_estimates\n",
    "\n",
    "\n",
    "estimates_method1 = run_simulation('random', 250, 100000, 30)\n",
    "estimates_method2 = run_simulation('lhs', 250, 100000, 30)\n",
    "estimates_method3 = run_simulation('orthogonal', 250, 100000, 30)\n",
    "\n",
    "bootstrap_ci_method1 = bootstrap_confidence_interval(\n",
    "    np.array(estimates_method1))\n",
    "bootstrap_ci_method2 = bootstrap_confidence_interval(\n",
    "    np.array(estimates_method2))\n",
    "bootstrap_ci_method3 = bootstrap_confidence_interval(\n",
    "    np.array(estimates_method3))\n",
    "\n",
    "stat, p_value = hypothesis_testing(\n",
    "    np.array(estimates_method1), np.array(estimates_method2))\n",
    "\n",
    "print(f\"Bootstrap CI for Method 1: {bootstrap_ci_method1}\")\n",
    "print(f\"Bootstrap CI for Method 2: {bootstrap_ci_method2}\")\n",
    "print(f\"Stat: {stat}, P-value: {p_value}\")\n",
    "\n",
    "stat, p_value = hypothesis_testing(\n",
    "    np.array(estimates_method1), np.array(estimates_method3))\n",
    "\n",
    "print(f\"Bootstrap CI for Method 1: {bootstrap_ci_method1}\")\n",
    "print(f\"Bootstrap CI for Method 3: {bootstrap_ci_method3}\")\n",
    "print(f\"Stat: {stat}, P-value: {p_value}\")\n",
    "\n",
    "stat, p_value = hypothesis_testing(\n",
    "    np.array(estimates_method2), np.array(estimates_method3))\n",
    "\n",
    "print(f\"Bootstrap CI for Method 2: {bootstrap_ci_method2}\")\n",
    "print(f\"Bootstrap CI for Method 3: {bootstrap_ci_method3}\")\n",
    "print(f\"Stat: {stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef55ee20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:04:56.718181100Z",
     "start_time": "2023-11-19T18:03:26.474342200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calvin\n",
    "c_points, counts = mandelbrot_parallel_mc()\n",
    "# Assuming > 1 member of the set\n",
    "inside_points = c_points[counts == max(counts)]\n",
    "\n",
    "max_iter = 250\n",
    "total_area = (X_LIM[1] - X_LIM[0]) * (Y_LIM[1] - Y_LIM[0])\n",
    "\n",
    "# Calculate the PDF from the points obtained in the initial simulation\n",
    "\n",
    "\n",
    "def compute_importance_sampling_distribution(points, bins=10000, epsilon=1e-5):\n",
    "    hist, _, _ = np.histogram2d(\n",
    "        points.real, points.imag, bins=bins, range=[X_LIM, Y_LIM])\n",
    "    # Adding a small constant to avoid zero probabilities\n",
    "    pdf = hist / np.sum(hist) + epsilon\n",
    "    return pdf / np.sum(pdf)  # Normalizing\n",
    "\n",
    "\n",
    "def generate_samples_with_importance_sampling(pdf, num_samples):\n",
    "    x_edges = np.linspace(*X_LIM, len(pdf[0]) + 1)\n",
    "    y_edges = np.linspace(*Y_LIM, len(pdf) + 1)\n",
    "\n",
    "    # Create a 2D grid of indices\n",
    "    x_indices, y_indices = np.meshgrid(range(len(pdf[0])), range(len(pdf)))\n",
    "    flattened_indices = np.array([x_indices.flatten(), y_indices.flatten()]).T\n",
    "\n",
    "    # Choose random indices based on flattened PDF\n",
    "    chosen_indices = np.random.choice(\n",
    "        len(flattened_indices), size=num_samples, p=pdf.flatten())\n",
    "    # Map the sampled indices to complex numbers\n",
    "    sampled_points = np.array([complex(x_edges[x], y_edges[y])\n",
    "                              for x, y in flattened_indices[chosen_indices]])\n",
    "\n",
    "    return sampled_points\n",
    "\n",
    "\n",
    "def estimate_mandelbrot_area_with_importance_sampling(sampled_points):\n",
    "    points_inside_set = [point for point in sampled_points if mandelbrot(\n",
    "        point, max_iter) == max_iter]\n",
    "    area_estimate = len(points_inside_set) / len(sampled_points) * total_area\n",
    "    return area_estimate\n",
    "\n",
    "\n",
    "pdf = compute_importance_sampling_distribution(inside_points)\n",
    "new_samples = generate_samples_with_importance_sampling(pdf, 100000)\n",
    "area_estimate = estimate_mandelbrot_area_with_importance_sampling(new_samples)\n",
    "\n",
    "print(\n",
    "    f\"Estimated Mandelbrot set area with importance sampling: {area_estimate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5388871",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:05:23.915908900Z",
     "start_time": "2023-11-19T18:04:56.724648300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calvin\n",
    "\n",
    "bin_sizes = [50, 100, 500, 1000, 5000, 10000]\n",
    "num_repetitions = 5\n",
    "mean_estimates = []\n",
    "std_devs = []\n",
    "\n",
    "for bins in tqdm(bin_sizes):\n",
    "    estimates = []\n",
    "    for _ in range(num_repetitions):\n",
    "        pdf = compute_importance_sampling_distribution(\n",
    "            inside_points, bins=bins)\n",
    "        new_samples = generate_samples_with_importance_sampling(pdf, 100000)\n",
    "        area_estimate = estimate_mandelbrot_area_with_importance_sampling(\n",
    "            new_samples)\n",
    "        estimates.append(area_estimate)\n",
    "\n",
    "    mean_estimates.append(np.mean(estimates))\n",
    "    std_devs.append(np.std(estimates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.errorbar(bin_sizes, mean_estimates, yerr=std_devs, fmt='-o',\n",
    "             capsize=5, elinewidth=2, markeredgewidth=2)\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Estimated area')\n",
    "plt.title('Mandelbrot set area estimates for different bin sizes')\n",
    "plt.xscale('log')\n",
    "plt.yscale('linear')\n",
    "plt.grid(True)\n",
    "plt.savefig(f'{SAVE_DIR}/different_bin_sizes.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d301e7a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:07:05.550678300Z",
     "start_time": "2023-11-19T18:05:23.908802600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calvin\n",
    "\n",
    "sample_sizes = [10000, 50000, 100000, 500000, 1000000]\n",
    "num_repetitions = 5\n",
    "mean_estimates = []\n",
    "std_devs = []\n",
    "\n",
    "for num_samples in tqdm(sample_sizes, desc='Sample Size Analysis', leave=True, position=0):\n",
    "    estimates = []\n",
    "    for _ in tqdm(range(num_repetitions), desc='Repetition', leave=False, position=1):\n",
    "        pdf = compute_importance_sampling_distribution(inside_points)\n",
    "        new_samples = generate_samples_with_importance_sampling(\n",
    "            pdf, num_samples)\n",
    "        area_estimate = estimate_mandelbrot_area_with_importance_sampling(\n",
    "            new_samples)\n",
    "        estimates.append(area_estimate)\n",
    "\n",
    "    mean_estimates.append(np.mean(estimates))\n",
    "    std_devs.append(np.std(estimates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.errorbar(sample_sizes, mean_estimates, yerr=std_devs,\n",
    "             fmt='-o', capsize=5, elinewidth=2, markeredgewidth=2)\n",
    "plt.xlabel('Number of samples')\n",
    "plt.ylabel('Estimated area')\n",
    "plt.title(\n",
    "    'Mandelbrot set area estimates of varying samples sizes\\nwith standard deviation')\n",
    "plt.xscale('log')\n",
    "plt.yscale('linear')\n",
    "plt.grid(True)\n",
    "plt.savefig(f'{SAVE_DIR}/varying_sample_sizes.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd0d923",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:08:52.974153400Z",
     "start_time": "2023-11-19T18:07:05.554680700Z"
    }
   },
   "outputs": [],
   "source": [
    "def variance_normal_mc(max_iter, n_points, repetitions):\n",
    "    estimates = []\n",
    "    for _ in range(repetitions):\n",
    "        c_points, counts = mandelbrot_parallel_mc(max_iter, n_points)\n",
    "        # Assuming > 1 member of the set\n",
    "        inside_points = c_points[counts == max(counts)]\n",
    "        area_estimate = len(inside_points) / n_points * total_area\n",
    "        estimates.append(area_estimate)\n",
    "    return np.var(estimates)\n",
    "\n",
    "\n",
    "def variance_importance_sampling(max_iter, n_points, repetitions):\n",
    "    pdf = compute_importance_sampling_distribution(inside_points)\n",
    "    estimates = []\n",
    "    for _ in range(repetitions):\n",
    "        new_samples = generate_samples_with_importance_sampling(pdf, n_points)\n",
    "        area_estimate = estimate_mandelbrot_area_with_importance_sampling(\n",
    "            new_samples)\n",
    "        estimates.append(area_estimate)\n",
    "    return np.var(estimates)\n",
    "\n",
    "\n",
    "sample_sizes = [10, 50, 100, 500, 1_000, 5_000, 10_000, 50_000, 100_000]\n",
    "repetitions = 10\n",
    "\n",
    "\n",
    "def variance_experiment(sample_sizes, repetitions, print_intermed=True):\n",
    "    variances_normal_mc = []\n",
    "    variances_importance_sampling = []\n",
    "\n",
    "    for n_points in tqdm(sample_sizes, desc='Sample Size Analysis', leave=True):\n",
    "        var_normal_mc = variance_normal_mc(max_iter, n_points, repetitions)\n",
    "        var_importance_sampling = variance_importance_sampling(\n",
    "            max_iter, n_points, repetitions)\n",
    "        variances_normal_mc.append(var_normal_mc)\n",
    "        variances_importance_sampling.append(var_importance_sampling)\n",
    "        if print_intermed:\n",
    "            print(\n",
    "                f\"Sample Size: {n_points}, Variance Normal MC: {var_normal_mc}, Variance Importance Sampling: {var_importance_sampling}\")\n",
    "\n",
    "    return variances_normal_mc, variances_importance_sampling\n",
    "\n",
    "\n",
    "variances_normal_mc, variances_importance_sampling = variance_experiment(\n",
    "    sample_sizes, repetitions, print_intermed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.semilogx(sample_sizes, variances_normal_mc, label='Normal Monte Carlo')\n",
    "plt.semilogx(sample_sizes, variances_importance_sampling,\n",
    "             label='Importance Sampling')\n",
    "plt.xlabel('Number of samples')\n",
    "plt.ylabel('Variance of area estimates')\n",
    "plt.title('Comparison of variance across sample sizes')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\n",
    "    f'{SAVE_DIR}/comparison_of_variance_across_sample_sizes.png', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
